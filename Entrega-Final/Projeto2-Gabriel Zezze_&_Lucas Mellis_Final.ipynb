{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 19/Set até às 23:59.<br />\n",
    "Grupo: 2 ou 3 pessoas - grupos com 3 pessoas terá uma rubrica diferenciada.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO gravar a key do professor no arquivo**\n",
    "\n",
    "\n",
    "### Entrega Intermediária: Check 1 - APS 2\n",
    "\n",
    "Até o dia 10/Set às 23:59, xlsx deve estar no Github com as seguintes evidências: \n",
    "\n",
    "  * Produto escolhido.\n",
    "  * Arquivo Excel contendo a base de treinamento e a base de testes já classificadas.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Parte I - Adquirindo a Base de Dados\n",
    "\n",
    "Acessar o notebook **Projeto-2-Planilha** para realizar a coleta dos dados. O grupo deve classificar os dados coletados manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Parte II - Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Não se esqueça de implementar o Laplace Smoothing (https://en.wikipedia.org/wiki/Laplace_smoothing).\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n",
    "Escreva o seu código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from emoji import UNICODE_EMOJI\n",
    "Data = pd.read_excel(\"tweets_rappi.xlsx\")\n",
    "relevante = Data.Treinamento[Data.Classifcação == 'relevante']\n",
    "irrelevante = Data.Treinamento[Data.Classifcação == 'irrelevante']\n",
    "Probabilidades = Data['Classifcação'].value_counts(normalize=True)\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "                                        #Funções de limpeza de tweets#\n",
    "#-Função Limpeza para uma serie de tweets#\n",
    "def LimpezaPrevia(tweets):\n",
    "    acentos = {'á':'a','ã':'a','é':'e','ê':'e','í':'i','ô':'o','ó':'o','ú':'u','õ':'o','Z':'Zz14'}\n",
    "    words = []\n",
    "    for i in tweets:\n",
    "        a = i.replace('/n','')\n",
    "        b = a.replace('!','')\n",
    "        c = b.replace(\"'\",\"\")\n",
    "        d = c.replace(\",\",\"\")\n",
    "        for y in UNICODE_EMOJI.keys():\n",
    "            if y in d:\n",
    "                e = d.replace(y,\" {} \".format(y))\n",
    "                lista = e.split(\" \")\n",
    "                break\n",
    "            else:\n",
    "                lista = d.split(\" \")\n",
    "        for x in lista:\n",
    "            if '@' in x:\n",
    "                del x\n",
    "            elif 'http' in x:\n",
    "                del x\n",
    "            elif x == 'rt':\n",
    "                  del x\n",
    "            elif '\\n' in x:\n",
    "                del x\n",
    "            else:\n",
    "                z = list(x)\n",
    "                for h in z:\n",
    "                    if h == '?':\n",
    "                        z.remove(h)\n",
    "                    elif h == ',':\n",
    "                        z.remove(h)\n",
    "                    elif h == '.':\n",
    "                        z.remove(h)\n",
    "                    elif h == '!':\n",
    "                        z.remove(h)\n",
    "                    elif h == '#':\n",
    "                        z.remove(h)\n",
    "                    elif h == '&':\n",
    "                        z.remove(h)\n",
    "                    elif h == ';':\n",
    "                        z.remove(h)\n",
    "                    elif h == '(':\n",
    "                        z.remove(h)\n",
    "                    elif h == ')':\n",
    "                        z.remove(h)\n",
    "                    elif h == '\"':\n",
    "                        z.remove(h)\n",
    "                    elif h == '/':\n",
    "                        z.remove(h)\n",
    "                    elif h == '*':\n",
    "                        z.remove(h)\n",
    "                    elif h == '-':\n",
    "                        z.remove(h)\n",
    "                    elif h in acentos:#RETIRANDO ACENTOS MAIS USADOS#\n",
    "                        w = acentos[h]\n",
    "                        z.remove(h)\n",
    "                        z.append(w)\n",
    "                y = ''.join(z)\n",
    "                words.append(y)\n",
    "    return words\n",
    "#-Função Limpeza de um tweet:\n",
    "def LimpezaLinha(lista):\n",
    "    lista2 = []\n",
    "    acentos = {'á':'a','ã':'a','é':'e','ê':'e','í':'i','ô':'o','ó':'o','ú':'u','õ':'o'}\n",
    "    a = lista.replace('/n','')\n",
    "    b = lista.replace('!','')\n",
    "    c = lista.replace(\"'\",\"\")\n",
    "    for y in UNICODE_EMOJI.keys():\n",
    "        if y in c:\n",
    "            d = c.replace(y,\" {} \".format(y))\n",
    "            lista3 = d.split(\" \")\n",
    "            break\n",
    "        else:\n",
    "            lista3 = c.split(\" \")\n",
    "    for x in lista3:\n",
    "        if '@' in x:\n",
    "            del x\n",
    "        elif 'http' in x:\n",
    "            del x\n",
    "        elif x == 'rt':\n",
    "            del x\n",
    "        elif '\\n' in x:\n",
    "            del x\n",
    "        else:\n",
    "            z = list(x)\n",
    "            for h in z:\n",
    "                if h == '?':\n",
    "                    z.remove(h)\n",
    "                elif h == ',':\n",
    "                    z.remove(h)\n",
    "                elif h == '.':\n",
    "                    z.remove(h)\n",
    "                elif h == '!':\n",
    "                    z.remove(h)\n",
    "                elif h == '#':\n",
    "                    z.remove(h)\n",
    "                elif h == '&':\n",
    "                    z.remove(h)\n",
    "                elif h == ';':\n",
    "                    z.remove(h)\n",
    "                elif h == '(':\n",
    "                    z.remove(h)\n",
    "                elif h == ')':\n",
    "                    z.remove(h)\n",
    "                elif h == '\"':\n",
    "                    z.remove(h)\n",
    "                elif h == '/':\n",
    "                    z.remove(h)\n",
    "                elif h == '*':\n",
    "                    z.remove(h)\n",
    "                elif h == '-':\n",
    "                    z.remove(h)\n",
    "                elif h in UNICODE_EMOJI:\n",
    "                    h.replace(h,' {} '.format(h))\n",
    "                elif h in acentos:\n",
    "                    w = acentos[h]\n",
    "                    z.remove(h)\n",
    "                    z.append(w)\n",
    "            y = ''.join(z)\n",
    "            lista2.append(y)\n",
    "    return lista2\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "            #Dicionario com palavras relevantes e suas respectivas probabilidades sem LaPlace Smoothing#\n",
    "#-Dicionario de palavres relevantes e probabilidades sem la place smoothing:\n",
    "palavrasRel = LimpezaPrevia(relevante)\n",
    "Relevantes = pd.DataFrame(palavrasRel)\n",
    "PRel = Relevantes[0].value_counts().to_dict()\n",
    "#-Dicionario de palavres irrelevantes e probabilidades sem la place smoothing:\n",
    "palavrasIrrel = LimpezaPrevia(irrelevante)\n",
    "Irrelevantes = pd.DataFrame(palavrasIrrel)\n",
    "PIrrel = Irrelevantes[0].value_counts().to_dict()\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "                #Criando lista com todas as palavras da base de treinamento sem repetições#\n",
    "listaTotal = list(set().union(palavrasRel, palavrasIrrel))\n",
    "PalavrasTotal = len(listaTotal)\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "                                    #Probabilidade com LaPlace Smoothing#\n",
    "#-Probabilidade de palavras relevantes:\n",
    "for i in PRel.keys():\n",
    "    PRel[i] = (PRel[i]+1)/(len(Relevantes)+PalavrasTotal)\n",
    "#-Probabilidade de Palavras Irrelevantes:\n",
    "for i in PIrrel.keys():\n",
    "    PIrrel[i] = (PIrrel[i]+1)/(len(Irrelevantes)+PalavrasTotal)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "                                    #Algoritmo de Classificação#\n",
    "##Função a qual recebe uma Data Series de tweets, dicioinario de palavras relevantes com respectivas probabilidade e o -\n",
    "#- mesmo para palavras irrelevantes , a quantidade totais de palavras nos dois dicionarios juntos e por fim uma Data-\n",
    "#-Serie com as probabilidades de ser relevante ou irrelevante##\n",
    "def ClassificadorNaiveBayes(tweets,palavrasRelevantes,palavrasIrrelevantes,palavrasTotais,Probabilidades):\n",
    "    listaResultado = []\n",
    "    for i in tweets:\n",
    "        palavrasLimpas = LimpezaLinha(i)\n",
    "        P_Relevante = 1\n",
    "        P_Irrelevante = 1\n",
    "        for z in palavrasLimpas:\n",
    "            if z in palavrasRelevantes:\n",
    "                P_Relevante *= palavrasRelevantes[z]\n",
    "            else:\n",
    "                P_Relevante *= 1/(palavrasTotais+len(palavrasRelevantes))\n",
    "        for y in palavrasLimpas:\n",
    "            if y in palavrasIrrelevantes:\n",
    "                P_Irrelevante *= palavrasIrrelevantes[y]\n",
    "            else:\n",
    "                P_Irrelevante *= 1/(palavrasTotais+len(palavrasIrrelevantes))\n",
    "        P_Relevante *= Probabilidades[1]\n",
    "        P_Irrelevante *= Probabilidades[0]\n",
    "        if P_Relevante > P_Irrelevante:\n",
    "            listaResultado.append(\"relevante\")\n",
    "        elif P_Relevante < P_Irrelevante:\n",
    "            listaResultado.append(\"irrelevante\")\n",
    "        else:\n",
    "            listaResultado.append(\"EQUIVALENTE\")\n",
    "    SerieFinal = pd.Series(listaResultado,name=\"Classificação_NaiveBayes\")\n",
    "    return SerieFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Obrigatório para grupos de 3 alunos:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequencia de acontecimento</th>\n",
       "      <th>Porcentagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positivos_falsos</th>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positivos_verdadeiros</th>\n",
       "      <td>13</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negativos_verdadeiros</th>\n",
       "      <td>149</td>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negativos_falsos</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Frequencia de acontecimento  Porcentagem\n",
       "positivos_falsos                                30         15.0\n",
       "positivos_verdadeiros                           13          6.5\n",
       "negativos_verdadeiros                          149         74.5\n",
       "negativos_falsos                                 8          4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataTeste = pd.read_excel('tweets_rappi_teste.xlsx')\n",
    "TweetsTeste = DataTeste['Teste']\n",
    "x = ClassificadorNaiveBayes(TweetsTeste,PRel,PIrrel,PalavrasTotal,Probabilidades)\n",
    "Data2 = DataTeste.join(x)\n",
    "ClassificacaoManual = Data2['Classificação']\n",
    "ClassificacaoNaiveBayes = Data2['Classificação_NaiveBayes']\n",
    "def ResultadoClassificador(Manual,NaiveBayes):\n",
    "    x = 0\n",
    "    listaPor100 = []\n",
    "    listaResultado = {\"positivos_falsos\":0,\"positivos_verdadeiros\":0,\"negativos_verdadeiros\":0,\"negativos_falsos\":0}\n",
    "    for i in Manual:\n",
    "        if i == 'irrelevante':\n",
    "            if NaiveBayes[x] == i:\n",
    "                listaResultado['negativos_verdadeiros'] += 1\n",
    "                x += 1\n",
    "            elif NaiveBayes[x] != i:\n",
    "                listaResultado['negativos_falsos'] += 1\n",
    "                x += 1\n",
    "        elif i == 'relevante':\n",
    "            if NaiveBayes[x] == i:\n",
    "                listaResultado['positivos_verdadeiros'] += 1\n",
    "                x += 1\n",
    "            elif NaiveBayes[x] != i:\n",
    "                listaResultado['positivos_falsos'] += 1\n",
    "                x += 1\n",
    "    Final = pd.DataFrame.from_dict(listaResultado,orient='index',columns=['Frequencia de acontecimento'])\n",
    "    for i in Final['Frequencia de acontecimento']:\n",
    "        listaPor100.append((i/200)*100)\n",
    "    Final['Porcentagem'] = listaPor100\n",
    "    return Final\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "                            #Resultado da Função Classificadora com porcentagem de resultados#\n",
    "ResultadoClassificador(ClassificacaoManual,ClassificacaoNaiveBayes)\n",
    "#lista = ['o atendimento do rappi e tao rapido quanto o do sus']\n",
    "#ClassificadorNaiveBayes(lista,PRel,PIrrel,PalavrasTotal,Probabilidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando o classificador NaiveBayes e sua utilização\n",
    "Nas celulas de codigo acima foi programado um classificador de sentimento NaiveBayes de tweets sobre o aplicativo Rappi utilizando uma base de treinamento de 500 tweets a qual foi primeiramente classificada como relevante ou irrelevante manualmente pela dupla autora deste projeto, o conceito de relevancia utilizado foi caso o tweet contesse alguma informação a qual pode ser possivelmente interpretada como um feedback positivo ou negativo sobre o aplicativo e seu serviço. Posteriormente a base de tweets de treinamento foi lida pelo python através da bliblioteca Pandas a qual possibilitou separar os tweets de treinamento em dois grupos classificados manualmente como relevantes e os classificados manualmente como irrelevante e através deste dois grupos foi possivel criar duas listas de palvras com suas respectivas probabilidades de aparecerem em tweets relevantes e de aparecerem em tweets irrelevantes, já considerando a tecnica de \"Laplace Smoothing\" a qual irá punir a classificação caso uma palavra no tweet a ser classificado não esteja disponivel no banco de palvras obtidas do treinamento. Após criar as duas listas de palavras citadas acima, foi utilizado outra base de tweets a qual foi classificada manualmente mas será tambem classificada pelo classificador de sentimento NaiveBayes programado nas celulas de codigo acima, após a classificação dos tweets pelo classificador NaiveBayes, estas classificações foram comparadas à classificação manual previamente feita e assim separadas em 4 grupos: positivos falsos(tweets classificados pelo classificador NaiveBayes como relevante à empresa mas foram classificados manualmente como irrelevantes),positivos verdadeiros(tweets classificados pelo classificador como relevantes e também classificado manualmente como relevante),negativos falsos(tweets classificados pelo classificador como irrelevantes mas classificados manualmente como relevantes) e por fim o quarto grupo os negativos verdadeiros(tweets classificados como irrelevante pelo classificador e pela classificação manual).\n",
    "\n",
    "#### Resultados da utilização do classificador em uma base de tweets teste\n",
    "Após o  término da comparação entre a classificação manual da base de tweets de teste com a classificacao do clasficador NaiveBayes da mesma base de tweets obtivemos os seguintes resultados: 31 ocorrencias de positivos falsos(15.5%), 12 ocorrencias de positivos verdadeiros(6%), 149 ocorrencias de negativos verdadeiros(74.5%) e por fim 8 ocorrencias de negativos falsos(4%).Observando esses resultaddos pode-se concluir que o classificador possui uma alta capacidade de classificar corretamente tweets irrelevantes, pois de 200 tweets utilizados na base de teste ele classificou 149 tweets corretamente como irrelevante, apesar de classificar 31 tweets os quais eram irrelevantes como relevantes. Além disso é possivel concluir também que para tweets relevantes o classificador classificou 12 tweets corretamente mas deixou de classificar 8 tweets dos quais eram relevantes, classificando-os como irrelevantes. No entanto 8 tweets dos quais eram relevante classificados erroneamente como irrelevantes representam 6% de todos os tweets e este numero  pode ser considerada relativamente baixo, mostrando que o classificador tem uma baixa tendencia de classificar tweets relevantes como irrelevantes mostrando ser eficiente em selecionar tweets os quais seriam relevantes para a empresa, sem deixar muitos tweets os quais seriam relevantes \"escapar\".\n",
    "\n",
    "#### Classificador em relação a dupla negação e sarcasmo\n",
    "O classificador claramente é capaz de classificar tweets, no entanto por se tratar de um classificador Naive, cada palavra do tweet será interpretada independentemente da frase na qual esta contida, ou seja, nos tweets em  que há uma dupla negação como por exemplo \"Eu não odeio o rappi\", a classificação deste tweet dependerá somente das palavras contidas nele e não do real significado do tweet, pois o classificador não analisa a semântica da frase. Além disso, o conceito de relevante utilizado para o treinamento do classificador foi que para um tweet ser relevante ele deve conter alguma informação a qual pode ser interpretada como um feedback para empresa, ou seja, somente a probabilidade de cada palavra individualmente ser ou nao relevante irá determinar sua classificação. Já para tweets que apresentam sarcasmo a classificação do classificador não condiz com a classificação correta, pois cada palavras em um tweet sarcastico tem seu propris significado, mas se analisadas em conjunto apresentam o sentido ironico. No entanto o classificador, como dito antes, utiliza a tecnica de analisar cada palavra individualmente, não conseguindo compreender o verdadeiro significado da frase e causando assim, uma classificação erronea.\n",
    "\n",
    "#### Por que o classificador deve ser continuado e incentivado?\n",
    "O classificador além da capacidade de classificar tweets como relevantes ou não, possui um grande potencial no mundo corporativo pois através deste classificador é possivel classificar tweets retirados da internet para serem analisados pela empresa assim sendo possivel encontrar uma fonte de feedback na qual pode guiar a empresa em suas decisões e apresentar a opinião do publico geral sobre o produto/serviço,  possibilitando também analisar a reação do publico a uma decisão ou ação da empresa bastando somente retirar tweets os quais foram postados após o anuncio de tal decisão/ação podendo ter uma fonte rapida e eficaz de feedback para validar sua decisão, alem disso caso o desenvolvimento do classificador se mantenha, é possivel criar uma maneira na qual tweets são baixados e classificados mensalmente criando uma fonte de feedback constante do produto/serviço a qual permite que a empresa tome decisões melhores com base na opinião do publico mais recente.\n",
    "\n",
    "#### Realimentação da base de treinamento \n",
    "Alimentar a base de treinamento com classificações e tweets classificados pelo próprio classificador não é correto e não treinará o classificador corretamente pois após o primeiro treinamento o classificador erra em algumas classificações e re-alimentar o treinamento deste classificador com estas classificações erradas causará uma propagação deste erro tornando-o cada vez maior a cada treinamento piorando as classificaçõe e provocando a cada re-alimentação um maior indice de erros na classificação.\n",
    "\n",
    "#### Cenarios alternativos de utilização do classificador NaiveBayes\n",
    "Além da análise sentimental feita acima, o classificador NaiveBayes pode ser útil para vários outros objetivos. O primeiro dele diz respeito ao filtro de spam, já que você pode passar para ele uma base de emails ou mensagens consideradas spam e o classificador aprenderá com elas e poderá classificar novos emails ou mensagens recebidas em spam ou não, e assim, não deixará que seu computador fique com uma caixa de spam lotada de emails indesejados. Um segundo uso é o sistema de recomendação, pois ele pode ser combinado a ele uma filtragem colaborativa e assim prever, com base no machine learning e na leitura de dados, se um usuário gostaria ou nao de um produto 'x'.\n",
    "\n",
    "#### Propostas para refinar e melhorar o classificador NaiveBayes\n",
    "O classificador classifica tweets com uma margem de erro relativamente baixa no entanto é possivel melhorar o classificador através de implementações ao codigo, como por exemplo aumentar a base de tweets para treinamento, remover pronomes pessoais através de metodos em Python como replace(https://www.w3schools.com/python/ref_string_replace.asp) ou no caso de uma lista de palavras caso haja pronomes pessoais dentro desta lista, retiralos com uma iteração dentro da lista e deletando o item caso seja um pronome pessoal usando o metodo del ou remove(https://www.tutorialspoint.com/python/list_remove.htm), outra implementação na qual melhoraria o classificador seria agrupar diferentes formas de uma mesma palavra e um grupo o qual seria considerado somente uma palavra, no python para isso seria nescessario o uso de uma bliblioteca chamada NLTK(https://www.tutorialspoint.com/python/python_stemming_and_lemmatization.htm) e por fim outra implementação ao codigo seria remover numeros com varios caracteres os quais na grande maioria não agregam nada, fazendo uma iteração na lista de palavras e usando o metodo \"try\" para tentar converter a string em um numero e caso a conversão tenha sucesso, usar o metodo del ou remove para remover este numero(https://stackoverflow.com/questions/1265665/how-can-i-check-if-a-string-represents-an-int-without-using-try-except).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
